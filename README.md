# DataMag360 ‚Äî Pipeline de Donn√©es Cloud Moderne

**DataMag360** est un projet de data engineering simulant une cha√Æne de traitement de donn√©es moderne et automatis√©e. Il vise √† d√©montrer la valeur d‚Äôune architecture cloud scalable de bout en bout.

## üéØ Objectifs

- G√©n√©rer des donn√©es de fr√©quentation et de transactions en magasin
- Exposer les donn√©es via une API REST (FastAPI)
- Automatiser la collecte horaire avec Apache Airflow
- Stocker les donn√©es dans Azure Blob Storage au format Parquet
- Ingestion et transformation via Azure Data Factory et Databricks
- Visualisation interactive via un tableau de bord Streamlit

## ‚ùì Pourquoi ce projet ?

De nombreuses entreprises disposent encore d'infrastructures obsol√®tes, avec peu ou pas d'automatisation, ce qui limite l'exploitation efficace des donn√©es. **DataMag360** propose une alternative moderne, bas√©e sur des outils cloud robustes : Azure, Databricks, Airflow, Streamlit.

## üß± Architecture Globale

Le pipeline couvre l‚Äôensemble du cycle de vie des donn√©es : g√©n√©ration ‚Üí ingestion ‚Üí transformation ‚Üí visualisation.

![pipeline](images_readme/pipeline_etl_datamag360.png)

---

## ‚öôÔ∏è √âtapes du pipeline

### 1. G√©n√©ration des donn√©es

- Scripts Python (`sensor.py`, `realistic_sensor.py`) simulant le trafic en magasin
- Exposition via une API FastAPI avec deux endpoints principaux : `/visiteurs` et `/transactions`

![API - FastAPI](images_readme/postmanApi.PNG)

### 2. Orchestration API & stockage ‚Äî Airflow + Blob

- Ex√©cution horaire d‚Äôun DAG Apache Airflow
- Appel de l‚ÄôAPI FastAPI pour r√©cup√©rer les donn√©es simul√©es
- Sauvegarde des r√©sultats au format `.parquet` dans Azure Blob Storage

![DAG Airflow](images_readme/dag_airflow.PNG)

### 3. Ingestion vers SQL ‚Äî Azure Data Factory (ADF)

- Pipeline ADF d√©clench√© chaque heure
- Copie des fichiers `.parquet` depuis Blob vers Azure SQL Database (`raw schema`)
- Utilisation de param√®tres dynamiques (`date`, `hour`) pour lire les bons fichiers

![ADF ingestion](images_readme/adf_pl_ingest_raw.PNG)  
![trigger horaire](images_readme/hourly_trigger.PNG)

### 4. Transformation ‚Äî Azure Databricks

- Pipeline ADF quotidien d√©clenchant deux notebooks Databricks
- Nettoyage et traitement des donn√©es :
  - Suppression des valeurs nulles
  - Filtrage des valeurs aberrantes (ex. : trop faibles ou trop √©lev√©es, g√©n√©r√©es volontairement)
  - Remplacement par la **moyenne journali√®re** pour lisser les donn√©es dans le dashboard
- R√©sultats sauvegard√©s dans le `schema analytics` d‚ÄôAzure SQL

![ADF orchestrate notebooks](images_readme/adf_pl_daily_to_analytics.PNG)  
![Trigger nightly](images_readme/trigger_night_daily.PNG)

‚è± Pourquoi un d√©clenchement √† 3h du matin ?  
> Pour r√©duire les co√ªts Databricks (cluster inactif en journ√©e) et traiter les donn√©es de la veille sans conflit.

---

## üìä Dashboard Streamlit

- Filtres dynamiques : date, capteur, magasin, heure
- Visualisations claires des KPIs : trafic, chiffre d‚Äôaffaires, taux de conversion

![dashboard](images_readme/app_streamlit.PNG)

---

## ‚öôÔ∏è CI / CD

### CI ‚Äì Int√©gration Continue

- Formatage du code avec **Black**
- Tests automatiques avec **unittest**
- Pipeline GitHub Actions sur branches `dev` et `main`

### CD ‚Äì D√©ploiement Continu

- **FastAPI** automatiquement red√©ploy√©e sur Render √† chaque push

![render auto deploy](images_readme/autodeploye_render.PNG)

- **Streamlit App** h√©berg√©e sur Streamlit Cloud, mise √† jour automatiquement

---

## üß∞ Technologies utilis√©es

| Cat√©gorie          | Stack                                                   |
|--------------------|----------------------------------------------------------|
| Simulation         | Python, FastAPI                                          |
| Orchestration      | Apache Airflow, Azure Data Factory                       |
| Stockage           | Azure Blob Storage, Azure SQL                            |
| Traitement         | PySpark, SQL, Azure Databricks                           |
| Visualisation      | Streamlit                                                |
| CI/CD              | GitHub Actions, Render                                   |

---

## ‚ö†Ô∏è Sp√©cificit√©s techniques

- Conversion de fuseau horaire avec `addHours(utcNow(), 2)` pour se caler sur Paris
- `timestamp_insertion` ‚â† `date` pour √©viter les erreurs li√©es √† des valeurs corrompues ou manquantes
- Traitements **PySpark SQL** pour optimiser la scalabilit√©
- D√©ploiement **automatis√©** via Render + Streamlit Cloud

---

## ‚úÖ Conclusion

**DataMag360** est une solution compl√®te et modulaire qui illustre la puissance du cloud pour le traitement de la donn√©e.  
Du capteur jusqu‚Äôau dashboard, chaque √©tape est automatis√©e, scalable et adapt√©e √† une mise en production r√©elle.
