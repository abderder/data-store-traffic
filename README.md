	
üõçÔ∏è DataMag360 ‚Äî Une solution data compl√®te pour le suivi des magasins
> Toute l'activit√© d‚Äôun magasin, capt√©e, trait√©e et visualis√©e automatiquement.

DataMag360 est un projet de data engineering bout-en-bout simulant et analysant l‚Äôactivit√© de magasins physiques. Il automatise la g√©n√©ration, la collecte, le stockage, le traitement et la visualisation des donn√©es de fr√©quentation et de transactions. Le tout est orchestr√© dans un pipeline moderne, cloud et scalable.

üéØ Objectifs
- Simuler des donn√©es r√©alistes de visiteurs et de ventes
- Exposer ces donn√©es via une API REST
- Automatiser la collecte horaire avec Airflow
- Centraliser les fichiers dans Azure Blob Storage
- Charger les donn√©es dans Azure SQL (raw)
- Traiter et transformer les donn√©es via Databricks (analytics)
- Visualiser les KPIs dans une application Streamlit interactive

üõ†Ô∏è Stack Technique
| Domaine       | Outils / Technologies                                      |
|---------------|------------------------------------------------------------|
| G√©n√©ration    | Python, hashlib, random                                    |
| API           | FastAPI, Render (d√©ploiement)                             |
| Orchestration | Apache Airflow                                            |
| Stockage      | Azure Blob Storage (Parquet)                              |
| Ingestion     | Azure Data Factory (sch√©ma RAW - horaire)                 |
| Traitement    | Azure Databricks ‚Üí Azure SQL (analytics - quotidien)     |
| Visualisation | Streamlit, Plotly, Azure SQL                              |
| CI/CD         | GitHub Actions (lint + tests + d√©ploiement auto)         |

üìÇ Vue d‚Äôensemble du pipeline



üß™ Fonctionnalit√©s Cl√©s
- Donn√©es simul√©es horaires, r√©alistes et contr√¥l√©es
- API REST ouverte sans authentification
- DAG Airflow horaire automatis√©
- Partitionnement des donn√©es par date/hour
- Pipelines ADF pour ingestion (horaire) et transformation (quotidien)
- Base SQL Azure avec 2 sch√©mas : raw, analytics
- Visualisation des indicateurs cl√©s avec Streamlit
- D√©ploiements automatis√©s via CI/CD

üß∞ Pipeline en 7 √©tapes

1. G√©n√©ration des donn√©es
   - Fichiers sensor.py / realistic_sensor.py
   - Donn√©es synth√©tiques mais coh√©rentes

2. API REST FastAPI
   - Endpoints : /visiteurs et /transactions
   - Donn√©es disponibles par capteur, heure et jour

3. DAG Airflow (horaire)
   - Appelle l‚ÄôAPI FastAPI
   - Enregistre les r√©sultats en .parquet dans Azure Blob Storage

4. Pipeline ADF ‚Äì Ingestion (horaire)
   - Ingestion automatique des fichiers parquet vers Azure SQL (raw)

5. Pipeline ADF ‚Äì Transformation (quotidien)
   - Orchestration de notebooks Databricks
   - Nettoyage et transformation vers sch√©ma analytics

6. Base Azure SQL
   - 2 sch√©mas : raw (donn√©es brutes) et analytics (nettoy√©es et exploitables)

7. Dashboard Streamlit
   - Filtres interactifs, graphiques dynamiques, indicateurs cl√©s

‚öôÔ∏è Int√©gration Continue (CI) / D√©ploiement Continu (CD)

CI ‚Äì GitHub Actions
- Lint automatique avec Black
- Tests unitaires ex√©cut√©s √† chaque push / PR sur dev & main

CD ‚Äì D√©ploiement automatique (via int√©gration GitHub)
- L‚ÄôAPI FastAPI est automatiquement red√©ploy√©e par **Render** √† chaque push
- L‚Äôapp Streamlit est automatiquement mise √† jour via **Streamlit Cloud**

üîì Acc√®s
- API : https://datamag360.onrender.com
- App Streamlit : datamag360.streamlit.app

